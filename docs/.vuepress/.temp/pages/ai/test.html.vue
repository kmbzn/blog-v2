<template><div><p>네, 올려주신 강의 노트를 바탕으로 **&quot;Language Modeling과 RNN(Recurrent Neural Networks)&quot;**에 대해 핵심 내용을 짚어드리며 강의해 드리겠습니다.</p>
<p>마치 강의실에서 설명해 드리는 것처럼 편안한 어조로 진행하겠습니다. 시작해 볼까요?</p>
<hr>
<p>##1. Language Modeling: &quot;다음 단어 맞히기 게임&quot;자, 먼저 <strong>Language Modeling</strong>이 무엇인지부터 이야기해 봅시다. 아주 간단합니다.
여러분이 친구와 카톡을 할 때, 혹은 검색창에 글을 쓸 때 **&quot;다음에 올 단어가 무엇일까?&quot;**를 예측하는 것이 바로 Language Modeling입니다.</p>
<p>###공식적 정의와 수식수학적으로는 단어들의 시퀀스(Sequence) <span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x^{(1)}, x^{(2)}, \dots, x^{(t)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0824em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>가 주어졌을 때, 그다음 단어 <span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x^{(t+1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>이 나타날 확률 <span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mi mathvariant="normal">∣</mi><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x^{(t+1)}|x^{(t)}, \dots, x^{(1)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>를 계산하는 것입니다.</p>
<p>이걸 수행하는 시스템을 **Language Model(LM)**이라고 부르죠. 이 모델이 있으면 우리는 문장의 <strong>자연스러움</strong>을 평가할 수 있습니다.</p>
<ul>
<li>&quot;Jane went to the store.&quot; (자연스러움 \rightarrow 확률 High)</li>
<li>&quot;Store to Jane went the.&quot; (부자연스러움 \rightarrow 확률 Low)</li>
</ul>
<p>###N-Gram Language Models (고전적인 방식)딥러닝이 나오기 전에는 <strong>N-gram</strong>이라는 방식을 썼습니다. 아주 단순 무식하면서도 강력한 방법이죠.</p>
<ul>
<li><strong>아이디어:</strong> &quot;너무 먼 과거는 보지 말고, 바로 앞의 n-1개 단어만 보고 다음을 예측하자!&quot; (이걸 <strong>Markov Assumption</strong>이라고 합니다.)</li>
<li><strong>Unigram:</strong> 단어 1개만 봄 (&quot;the&quot;, &quot;students&quot;)</li>
<li><strong>Bigram:</strong> 2개 묶음 (&quot;the students&quot;)</li>
<li><strong>Trigram:</strong> 3개 묶음 (&quot;the students opened&quot;)</li>
<li><strong>4-gram:</strong> 4개 묶음 (&quot;the students opened their&quot;)</li>
</ul>
<p>예를 들어 4-gram 모델이라면, <code v-pre>students opened their</code> 뒤에 올 단어를 맞히기 위해 거대한 데이터(Corpus)에서 <code v-pre>students opened their</code> 뒤에 <code v-pre>books</code>가 몇 번 나왔는지, <code v-pre>exams</code>가 몇 번 나왔는지 <strong>숫자를 세서(Counting)</strong> 확률을 계산합니다.</p>
<p>하지만 이 방식은 한계가 명확합니다. 문맥을 n개까지만 볼 수 있고, 데이터에 없는 문장은 확률을 0으로 계산해 버리는 문제(Sparsity Problem)가 있죠.</p>
<hr>
<p>##2. Neural Language Models: 신경망의 등장그래서 등장한 것이 <strong>Neural Language Model</strong>입니다. 초기에는 <strong>Fixed-Window</strong> 방식을 썼습니다.</p>
<p>###고정 윈도우(Fixed-Window) 모델Bengio 교수님이 2000년에 제안한 방식인데, N-gram처럼 정해진 개수(Window)의 단어만 봅니다. 하지만 단어들을 <strong>One-hot vector</strong>가 아닌 <strong>Embedding Vector</strong>로 바꿔서 신경망에 넣는다는 점이 다릅니다.</p>
<ul>
<li><strong>장점:</strong> N-gram의 Sparsity 문제를 해결했습니다. (비슷한 단어는 비슷한 벡터 값을 가지니까요.)</li>
<li><strong>단점:</strong> 여전히 &quot;고정된 길이&quot;만 볼 수 있습니다. 문장이 길어져도 딱 정해진 앞부분만 보니 전체 맥락을 파악하기 힘들죠.</li>
</ul>
<hr>
<p>##3. RNN (Recurrent Neural Networks): &quot;기억을 가진 신경망&quot;자, 이제 오늘의 주인공 <strong>RNN</strong>이 등장합니다.
기존 신경망과 가장 큰 차이점은 **&quot;가변적인 길이의 입력을 처리할 수 있다&quot;**는 것입니다.</p>
<p>###핵심 아이디어 (Core Idea)RNN은 <strong>동일한 가중치(Weights W)를 매 시점(Timestep)마다 반복적으로 사용</strong>합니다.</p>
<ol>
<li>첫 번째 단어 <span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x^{(1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>이 들어오면, 은닉 상태(Hidden state) <span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>h</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">h^{(1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>을 만듭니다.</li>
<li>두 번째 단어 <span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x^{(2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>가 들어오면, 방금 만든 <span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>h</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">h^{(1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>과 현재 단어 <span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x^{(2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>를 같이 고려해서 <span v-pre class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>h</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">h^{(2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>를 만듭니다.</li>
<li>이 과정을 계속 반복합니다.</li>
</ol>
<p>이렇게 하면 이론적으로는 문장이 아무리 길어도, 아주 오래전 정보(Step 1의 정보)를 현재(Step t)까지 끌고 올 수 있습니다. 그리고 입력 길이가 길어져도 모델 사이즈(W)는 커지지 않죠. 아주 효율적입니다.</p>
<p>###RNN 학습 (Training)학습은 어떻게 할까요?</p>
<ol>
<li>거대한 텍스트 데이터(Big Corpus)를 준비합니다.</li>
<li>모델에 문장을 넣고, 매 순간 다음 단어를 예측하게 합니다 (\hat{\bm{y}}^{(t)}).</li>
<li>정답(실제 다음 단어 \bm{y}^{(t)})과 비교해서 오차(Cross-Entropy Loss)를 계산합니다.</li>
<li>이 오차를 줄이는 방향으로 **SGD(Stochastic Gradient Descent)**를 써서 가중치를 업데이트합니다.</li>
</ol>
<p>RNN으로 학습된 모델은 셰익스피어 소설 스타일이나 해리포터 스타일의 문장도 생성할 수 있습니다. 심지어 코드도 짤 수 있죠!</p>
<p>###RNN의 활용RNN은 단순히 다음 단어 예측뿐만 아니라 다양하게 쓰입니다.</p>
<ul>
<li><strong>Tagging:</strong> 품사 태깅 (이 단어가 명사인지 동사인지)</li>
<li><strong>Classification:</strong> 감성 분석 (이 문장이 긍정인지 부정인지)</li>
<li><strong>Translation:</strong> 기계 번역 (한국어 \rightarrow 영어)</li>
</ul>
<hr>
<p>##4. RNN의 변형들과 LSTM하지만 기본 RNN(Vanilla RNN)은 치명적인 단점이 있습니다. 문장이 너무 길어지면 앞부분의 정보가 뒷부분까지 전달되지 못하고 사라져 버리는 <strong>Vanishing Gradient Problem</strong>입니다.</p>
<p>이를 해결하기 위해 몇 가지 변형이 나옵니다.</p>
<p>###Bidirectional &amp; Multi-Layer RNNs* <strong>Bidirectional RNN:</strong> 문장을 앞에서 뒤로만 읽지 않고, <strong>뒤에서 앞으로도 읽습니다.</strong> 문맥을 양방향으로 파악하죠.</p>
<ul>
<li><strong>Multi-Layer RNN:</strong> RNN을 여러 층으로 쌓아서 더 복잡하고 깊은 패턴을 학습하게 합니다.</li>
</ul>
<p>###LSTM (Long Short-Term Memory)가장 중요한 해결책은 1997년에 나온 <strong>LSTM</strong>입니다. Vanishing Gradient 문제를 해결하기 위해 고안되었습니다.</p>
<p>LSTM의 핵심은 **&quot;Cell State (c^{(t)})&quot;**라는 고속도로를 뚫어주는 것입니다. 정보가 큰 방해 없이 쭉 흘러가게 하죠. 이를 조절하기 위해 3개의 **Gate(문)**을 둡니다.</p>
<ol>
<li><strong>Forget Gate (f^{(t)}):</strong> 이전 기억(c^{(t-1)}) 중 <strong>무엇을 잊을지</strong> 결정합니다. (Sigmoid 함수 사용: 0이면 삭제, 1이면 유지)</li>
<li><strong>Input Gate (i^{(t)}):</strong> 현재의 새로운 정보(\tilde{c}^{(t)}) 중 <strong>무엇을 기억에 더할지</strong> 결정합니다.</li>
<li><strong>Output Gate (o^{(t)}):</strong> 갱신된 기억을 바탕으로 <strong>현재 시점의 출력(h^{(t)})을 무엇으로 내보낼지</strong> 결정합니다.</li>
</ol>
<p>이 복잡해 보이는 구조 덕분에 LSTM은 아주 긴 문장에서도 주어와 동사의 호응 같은 장기 의존성(Long-term dependency)을 잘 잡아냅니다.</p>
<hr>
<p>##요약 (Summary)오늘 강의를 세 줄로 요약해 보겠습니다.</p>
<ol>
<li><strong>Language Model</strong>은 문맥을 보고 다음 단어의 확률을 예측하는 모델입니다. (N-gram에서 시작됨)</li>
<li><strong>RNN</strong>은 가변적인 길이의 입력을 처리하기 위해 <strong>동일한 가중치를 반복 적용</strong>하는 획기적인 신경망입니다.</li>
<li>RNN의 기억력 문제(Vanishing Gradient)를 해결하기 위해 <strong>Gate</strong>를 도입한 <strong>LSTM</strong>이 등장하여 현대 NLP의 기반이 되었습니다.</li>
</ol>
<p>이해가 좀 되셨나요? 복잡한 수식보다는 **&quot;정보가 어떻게 흐르고, 어떻게 기억되는가&quot;**에 초점을 맞추시면 훨씬 이해하기 쉬우실 겁니다.</p>
<p>혹시 LSTM의 수식 부분이나, RNN의 Backpropagation 과정에 대해 더 깊이 알고 싶으신가요?</p>
</div></template>


