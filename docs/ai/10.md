# 10. Machine Learning (1)

## What is (Machine) Learning?
- Agentê°€ ì„¸ìƒì„ ê´€ì°°í•œ í›„ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©´ í•™ìŠµí•˜ëŠ” ê²ƒ
- Agentê°€ ì»´í“¨í„°ì¼ ë•Œ machine learningì´ë¼ ë¶€ë¦„: ì»´í“¨í„°ê°€ dataë¥¼ ê´€ì°°í•˜ê³ , data ê¸°ë°˜ modelì„ êµ¬ì¶•í•˜ë©°, ì´ modelì„ ì„¸ìƒì— ëŒ€í•œ ê°€ì„¤ì´ì ë¬¸ì œ í•´ê²° softwareë¡œ ì‚¬ìš©
- Machineì´ í•™ìŠµí•´ì•¼ í•˜ëŠ” ì´ìœ 
    - ì„¤ê³„ì(í”„ë¡œê·¸ë˜ë¨¸)ê°€ ëª¨ë“  ê°€ëŠ¥í•œ ë¯¸ë˜ ìƒí™©ì„ ì˜ˆì¸¡í•  ìˆ˜ ì—†ìŒ
    - ì„¤ê³„ìê°€ í•´ê²°ì±…ì„ programí•˜ëŠ” ë°©ë²•ì„ ëª¨ë¥´ëŠ” ê²½ìš°
    - ëŒ€ë¶€ë¶„ì˜ ì‚¬ëŒë“¤ì€ ê°€ì¡± ì–¼êµ´ ì¸ì‹ì— ëŠ¥ìˆ™í•˜ì§€ë§Œ, ë¬´ì˜ì‹ì ìœ¼ë¡œ ìˆ˜í–‰
- ë³¸ ê³¼ì •ì—ì„œëŠ” ë‹¤ì–‘í•œ machine learning model (ì•Œê³ ë¦¬ì¦˜)ì„ íƒìƒ‰

## Three Types of Learning (1): Supervised Learning
- Inputê³¼ í•¨ê»˜ ì œê³µë˜ëŠ” ì„¸ ê°€ì§€ typeì˜ feedbackì´ ìˆìœ¼ë©°, ì´ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” í•™ìŠµ typeì„ ê²°ì •
- Supervised learning: Agentê°€ input-output ìŒì„ ê´€ì°°í•˜ê³  inputì—ì„œ outputìœ¼ë¡œ mappingí•˜ëŠ” í•¨ìˆ˜ë¥¼ í•™ìŠµ
    - ì˜ˆ: camera ì´ë¯¸ì§€(input)ì™€ "ë²„ìŠ¤", "ë³´í–‰ì" ë“±(output)ì˜ ìŒ
    - ì´ëŸ¬í•œ outputì„ labelì´ë¼ê³  í•¨
    - AgentëŠ” ìƒˆë¡œìš´ ì´ë¯¸ì§€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì ì ˆí•œ labelì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ë¥¼ í•™ìŠµ

![alt text](image-10.png)

## Classification vs. Regression
- Outputì´ ìœ í•œí•œ ê°’ ì§‘í•©(ì˜ˆ: sunny / cloudy / rainy ë˜ëŠ” true / false) ì¤‘ í•˜ë‚˜ì¼ ë•Œ, í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ classificationì´ë¼ í•¨
- Outputì´ ìˆ«ì(ì˜ˆ: ë‚´ì¼ì˜ ê¸°ì˜¨)ì¼ ë•Œ, í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ regressionì´ë¼ ë¶ˆë¦¼

![alt text](image-11.png)

## Three Types of Learning (2): Unsupervised Learning
- Unsupervised learning: Agentê°€ ëª…ì‹œì ì¸ feedback ì—†ì´ inputì˜ patternì„ í•™ìŠµ
- ê°€ì¥ ì¼ë°˜ì ì¸ unsupervised learning taskëŠ” clustering
  - ì ì¬ì ìœ¼ë¡œ ìœ ìš©í•œ input exampleì˜ cluster ê°ì§€
  - ì˜ˆ: Internetì—ì„œ ê°€ì ¸ì˜¨ ìˆ˜ë°±ë§Œ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë³´ê³ , computer vision systemì´ ì˜ì–´ ì‚¬ìš©ìê°€ "cats"ë¼ê³  ë¶€ë¥´ëŠ” ìœ ì‚¬í•œ ì´ë¯¸ì§€ì˜ í° clusterë¥¼ ì‹ë³„

![alt text](image-12.png)

## Three Types of Learning (3): Reinforcement Learning
- Reinforcement learning: Agentê°€ ì¼ë ¨ì˜ reinforcement (ë³´ìƒ ë° ì²˜ë²Œ)ë¡œë¶€í„° í•™ìŠµ
- ì˜ˆ: Chess ê²Œì„ì´ ëë‚œ í›„ agentëŠ” ì´ê²¼ìŒ(ë³´ìƒ) ë˜ëŠ” ì¡ŒìŒ(ì²˜ë²Œ)ì„ í†µë³´ë°›ìŒ
- Reinforcement ì´ì „ì— ì–´ë–¤ actionì´ ê°€ì¥ ì±…ì„ì´ ìˆëŠ”ì§€ ê²°ì •í•˜ê³ , ë¯¸ë˜ì— ë” ë§ì€ ë³´ìƒì„ ëª©í‘œë¡œ actionì„ ë³€ê²½í•˜ëŠ” ê²ƒì€ agentì˜ ëª«

![alt text](image-13.png)

## Difference between RL and SL
| Reinforcement learning | Supervised learning |
| :--- | :--- |
| Reinforcement learningì€ ìˆœì°¨ì ìœ¼ë¡œ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê²ƒ. ê°„ë‹¨íˆ ë§í•´, outputì€ í˜„ì¬ inputì˜ ìƒíƒœì— ë”°ë¼ ë‹¬ë¼ì§€ë©° ë‹¤ìŒ inputì€ ì´ì „ outputì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤ê³  ë§í•  ìˆ˜ ìˆìŒ. | Supervised learningì—ì„œëŠ”, ê²°ì •ì´ ì´ˆê¸° input ë˜ëŠ” ì²˜ìŒì— ì£¼ì–´ì§„ inputì— ë”°ë¼ ë‚´ë ¤ì§. |
| Reinforcement learningì—ì„œ ê²°ì •ì€ ì¢…ì†ì ì´ë¯€ë¡œ, ì¢…ì†ì ì¸ ê²°ì •ì˜ sequenceì— labelì„ ë¶€ì—¬ | Supervised learningì—ì„œ ê²°ì •ì€ ì„œë¡œ ë…ë¦½ì ì´ë¯€ë¡œ, ê° ê²°ì •ì— labelì´ ì£¼ì–´ì§. |
| ì˜ˆ: Chess game | ì˜ˆ: Object recognition |

## Self-Supervised Learning
- ì´ì „ ì„¸ ê°€ì§€ ë²”ì£¼í™” ì¸¡ë©´ì—ì„œ unsupervised learningì— ì†í•¨
- Self-supervised learning (SSL) methodëŠ” downstream taskì— ì¢‹ì€ featureë¥¼ ìƒì„±í•˜ëŠ” "pretext" taskë¥¼ í•´ê²°
    - Pretext: êµ¬ì‹¤, í•‘ê³„
    - Supervised learning objective (ì˜ˆ: classification, regression)ë¡œ í•™ìŠµ
    - ì´ëŸ¬í•œ pretext taskì˜ labelì€ ìë™ìœ¼ë¡œ ìƒì„±ë¨
- ì˜ˆ: ì´ë¯¸ì§€ ë³€í˜• ì˜ˆì¸¡ í•™ìŠµ / ì†ìƒëœ ì´ë¯¸ì§€ ì™„ì„± í•™ìŠµ
- SSLì˜ ê°•ì 
    - Pretext task í•´ê²°ì„ í†µí•´ modelì´ ì¢‹ì€ featureë¥¼ í•™ìŠµ ê°€ëŠ¥
    - Pretext taskë¥¼ ìœ„í•œ labelì„ ìë™ìœ¼ë¡œ ìƒì„± ê°€ëŠ¥
- SSLì˜ ì˜ë¯¸
    - ì¼ë°˜ì ìœ¼ë¡œ self-supervised learning taskì˜ ì„±ëŠ¥ì—ëŠ” ê´€ì‹¬ì´ ì—†ìŒ (ì˜ˆ: modelì´ ì´ë¯¸ì§€ íšŒì „ ì˜ˆì¸¡ì„ ì™„ë²½í•˜ê²Œ í•™ìŠµí•˜ëŠ”ì§€ ì—¬ë¶€ëŠ” ì¤‘ìš”í•˜ì§€ ì•ŠìŒ)
    - í•™ìŠµëœ feature encoderë¥¼ downstream target taskì—ì„œ í‰ê°€

![alt text](image-14.png)

# Basic ML Algorithms

## Supervised learning: (ğ‘˜-)nearest neighbors

## Rain Prediction Problem
![alt text](image-15.png)

## Rain Prediction Problem â†’ Supervised Learning
- Supervised learning: Input-output ìŒì˜ data setì´ ì£¼ì–´ì§€ë©´, inputì„ outputì— mappingí•˜ëŠ” í•¨ìˆ˜ë¥¼ í•™ìŠµ

## Rain Prediction â†’ Supervised Learning â†’ Classification
- Classification: Input pointë¥¼ discrete categoryë¡œ mappingí•˜ëŠ” í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ëŠ” supervised learning task

## Nearest-Neighbor Classification
- Nearest-neighbor classification: Inputì´ ì£¼ì–´ì§€ë©´, í•´ë‹¹ inputì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ data pointì˜ classë¥¼ ì„ íƒí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜

## $k$-Nearest-Neighbors Classification
- $k$-nearest-neighbor classification: Inputì´ ì£¼ì–´ì§€ë©´, í•´ë‹¹ inputì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ $k$ê°œì˜ data point ì¤‘ ê°€ì¥ ì¼ë°˜ì ì¸ classë¥¼ ì„ íƒí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜

## $k$-Nearest-Neighbors Algorithm
- $k$-nearest neighbors algorithm ($k$-NN)ì€ classificationê³¼ regression ëª¨ë‘ë¥¼ ìœ„í•œ non-parametric method
- Input: Data setì—ì„œ $k$ê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ training example
- Training exampleì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ì°¨ì› feature spaceì˜ vectorì´ë©°, ê°ê° class labelì„ ê°€ì§
- Output
    - $k$-NN classification: Outputì€ class membership
    - $k$-NN regression: Outputì€ $k$ê°œ nearest neighbors ê°’ì˜ í‰ê· 

## Strengths and Weaknesses of $k$-NN Algorithm
- ê°•ì 
    - Trainingì´ (ê±°ì˜) í•„ìš” ì—†ìŒ: ì•Œê³ ë¦¬ì¦˜ì˜ training phaseëŠ” training sampleì˜ feature vectorì™€ class labelì„ ì €ì¥í•˜ëŠ” ê²ƒìœ¼ë¡œë§Œ êµ¬ì„±
    - ì´ê²ƒì´ non-parametric methodë¡œ ê°„ì£¼ë˜ëŠ” ì´ìœ 
- ì•½ì 
    - ê¸°ë³¸ "majority voting" classificationì˜ ë‹¨ì ì€ class distributionì´ í¸í–¥(skewed)ë  ë•Œ ë°œìƒ
    - ì¦‰, ë” ë¹ˆë²ˆí•œ classì˜ exampleì´ ìˆ˜ê°€ ë§ê¸° ë•Œë¬¸ì— $k$ nearest neighbors ì‚¬ì´ì—ì„œ ì¼ë°˜ì ì´ì–´ì„œ ìƒˆë¡œìš´ exampleì˜ ì˜ˆì¸¡ì„ ì§€ë°°í•˜ëŠ” ê²½í–¥
    - ì´ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ëŠ” í•œ ê°€ì§€ ë°©ë²•ì€ test pointì—ì„œ ê° $k$ nearest neighborsê¹Œì§€ì˜ ê±°ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ classificationì— ê°€ì¤‘ì¹˜(weight)ë¥¼ ì£¼ëŠ” ê²ƒ (weighted nearest neighbors)
    - Parameter ì„ íƒ: ìµœì ì˜ $k$ ì„ íƒì€ dataì— ë”°ë¼ ë‹¤ë¦„. ì¢‹ì€ $k$ëŠ” ë‹¤ì–‘í•œ heuristic techniqueì„ í†µí•´ ì„ íƒë˜ì–´ì•¼ í•¨

## Is It Class A or Class B?

![alt text](image-16.png)

# Basic ML Algorithmss
### Unsupervised learning: ğ‘˜-means clustering

## $k$-Means Clustering Algorithm
![alt text](image-17.png)

- $n$ê°œì˜ observation ë˜ëŠ” data point ì§‘í•© $(\mathbf{x}_1,~\mathbf{x}_2,~\dots,~\mathbf{x}_n)$ì´ ì£¼ì–´ì¡Œì„ ë•Œ (ê° $\mathbf{x}_i$ëŠ” $d$-ì°¨ì› vector), $k$-means clusteringì€ $n$ê°œì˜ observationì„ $k(\le n)$ê°œì˜ ì§‘í•© $\mathbf{S} = \{S_1,~S_2,~\dots,~S_k\}$ë¡œ ë¶„í• í•˜ì—¬ WCSS (within-cluster sum of squares), ì¦‰ varianceë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨
- ê³µì‹ì ìœ¼ë¡œ, ëª©í‘œëŠ” ë‹¤ìŒì„ ì°¾ëŠ” ê²ƒ:
$$ \underset{\mathbf{S}}{\operatorname{argmin}} \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_i} \left\| \mathbf{x} - \boldsymbol{\mu}_i \right\|^2 $$
- ì—¬ê¸°ì„œ $\boldsymbol{\mu}_i$ëŠ” $S_i$ì— ìˆëŠ” pointë“¤ì˜ í‰ê· 
- $k$-means ì•Œê³ ë¦¬ì¦˜ì€ Expectation-Maximization (EM) ì•Œê³ ë¦¬ì¦˜ì˜ instance
- ì´ˆê¸° $k$ê°œì˜ í‰ê·  ì§‘í•© $\boldsymbol{\mu}_1^{(1)},~\dots,~\boldsymbol{\mu}_k^{(1)}$ì´ ì£¼ì–´ì§€ë©´, ì•Œê³ ë¦¬ì¦˜ì€ ë‘ ë‹¨ê³„ë¥¼ ë²ˆê°ˆì•„ ìˆ˜í–‰
- Assignment (Expectation) step: ê° data pointë¥¼ ê°€ì¥ ê°€ê¹Œìš´ í‰ê· (least squared Euclidean distance)ì„ ê°€ì§„ clusterì— í• ë‹¹
    $$ S_i^{(t)} = \{ \mathbf{x}_p : \left\| \mathbf{x}_p - \boldsymbol{\mu}_i^{(t)} \right\|^2 \le \left\| \mathbf{x}_p - \boldsymbol{\mu}_j^{(t)} \right\|^2 \quad \forall j,~1 \le j \le k \} $$
    - ì—¬ê¸°ì„œ ê° $\mathbf{x}_p$ëŠ” ì •í™•íˆ í•˜ë‚˜ì˜ $S^{(t)}$ì— í• ë‹¹ë¨
- Update (Maximization) step: ê° clusterì— í• ë‹¹ëœ data pointë“¤ì˜ (centroid) í‰ê· ì„ ë‹¤ì‹œ ê³„ì‚°
    $$ \boldsymbol{\mu}_i^{(t+1)} = \frac{1}{|S_i^{(t)}|} \sum_{\mathbf{x}_p \in S_i^{(t)}} \mathbf{x}_p $$
- í• ë‹¹ì´ ë” ì´ìƒ ë³€ê²½ë˜ì§€ ì•Šì„ ë•Œ ì•Œê³ ë¦¬ì¦˜ì€ ìˆ˜ë ´
- ê·¸ëŸ¬ë‚˜ ì•Œê³ ë¦¬ì¦˜ì´ ìµœì (optimum)ì„ ì°¾ëŠ” ê²ƒì´ ë³´ì¥ë˜ì§€ëŠ” ì•ŠìŒ