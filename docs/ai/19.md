# 19. Language Modeling and Recurrent Neural Networks

# Language Modeling
Classic n-gram 모델

## Language Modeling
- Next word가 무엇인지 예측하는 Task
- 공식적 정의
    - Words sequence $x_1, x_2, \dots, x_t$가 주어졌을 때, Next word $x_{t+1}$의 Probability distribution 계산
    - $P(x_{t+1}|x_t, \dots, x_1)$
- 이를 수행하는 System을 Language Model이라 지칭

## Language Modeling
![alt text](image-58.png)
- Text 자체에 Probability를 할당하는 System으로 간주 가능
    - 예: $P(x_1, \dots, x_T) = P(x_1) \times P(x_2|x_1) \times \dots$
- Language Model의 활용
    - Score sentences: 문장의 자연스러움 평가 (예: 문법적으로 맞는 문장에 높은 점수 부여)
    - Generate sentences: Probability distribution에서 새로운 Word를 Sample하여 문장 생성

## You Use Language Models Every Day!
- 일상적인 검색 엔진, 자동 완성 기능 등에서의 활용 사례

## N-Gram Language Models
- Deep learning 이전의 근본적이고 고전적인 Language Model 구현 방식
- N-gram: $n$개의 연속적인 Words 덩어리
    - Unigrams, Bigrams, Trigrams, Four-grams 등
- Idea: 빈도 Statistics를 수집하여 Next word 예측에 활용

![alt text](image-58.png)

- Markov assumption
    - $x_{t+1}$은 오직 앞선 $n-1$개의 Words에만 의존한다고 가정
    - Assumption: $P(x_{t+1}|x_t, \dots, x_1) \approx P(x_{t+1}|x_t, \dots, x_{t-n+2})$
- Large corpus에서 개수를 세어(Counting) 확률 계산 (Statistical approximation)
    - Count ratio를 통해 Conditional probability 근사

## N-Gram Language Models: Example
- 4-gram Language Model 학습 예시
- Corpus 내 "students opened their" 뒤에 오는 단어들의 빈도를 확인하여 확률 부여
    - 예: "books" (0.4), "exams" (0.1)

## Generating Text with a N-Gram Language Model
- 간단한 trigram 언어 모델
    - 170만 이상의 단어 모음 (Reuters: 경제, 경영 뉴스)
    - 언어 모델로 텍스트를 생성할 수 있음.

![alt text](image-59.png)

![alt text](image-60.png)

# Neural Language Models

## A Fixed-Window Neural Language Model
![alt text](image-61.png)
- *Yoshua Bengio et al.* (2000)이 제안한 초기 버전
- N-gram Language Model 대비 개선점
    - Sparsity problem 없음
    - 관측된 모든 N-grams를 저장할 필요 없음
- 남은 문제점
    - Fixed window 크기가 너무 작음
    - Window 크기를 늘리면 Weights $W$가 커짐
    - $x_1$과 $x_2$가 서로 완전히 다른 Weights $W$에 곱해짐 (비대칭성)
- 임의의 길이인 Input을 처리할 수 있는 Neural architecture 필요성 대두

## Recurrent Neural Networks (RNN)
![alt text](image-62.png)
- Core idea: 동일한 Weights $W$를 반복적으로 적용

## A Simple RNN Language Model
![alt text](image-63.png)
- Recurrent 구조를 활용한 Language Model 도식

## RNN Language Models
- RNN의 장점
    - 모든 길이의 Input 처리 가능
    - 이론적으로 Step $t$의 연산에 아주 오래전 Step의 정보 활용 가능
    - Input context가 길어져도 Model size가 증가하지 않음
    - 모든 Timestep에 동일한 Weights를 적용하므로 Input 처리 방식에 대칭성 존재
- RNN의 단점
    - Recurrent computation 속도가 느림
    - 실제로는 먼 과거의 정보에 접근하기 어려움 (Vanishing gradient 등)

## Training an RNN Language Model
![alt text](image-64.png)
- 절차
    - Words sequence로 구성된 Big corpus 준비
    - RNN-LM에 입력하여 매 Step $t$마다 Output distribution $\hat{y}^{(t)}$ 계산
- Loss function
    - Predicted probability distribution $\hat{y}^{(t)}$와 True next word $y^{(t)}$ (One-hot) 간의 Cross-entropy
    - $J_t(\theta) = CE(y^{(t)}, \hat{y}^{(t)}) = -\sum y_w^{(t)} \log \hat{y}_w^{(t)} = -\log \hat{y}_{x_{t+1}}^{(t)}$
- 전체 Training set에 대해 Average loss 계산

## Training an RNN Language Model
- 전체 Corpus에 대해 한 번에 Loss와 Gradients를 계산하는 것은 비용이 과다함
- 실제로는 $x^{(1)}, \dots, x^{(T)}$를 Sentence (또는 Document) 단위로 처리
- Stochastic Gradient Descent (SGD)를 활용하여 작은 Chunk (Batch) 데이터에 대해 Loss 및 Gradient 계산 후 Weights update 반복

## Generating Text with an RNN Language Model
- 특정 Text style로 학습된 RNN-LM을 통해 해당 스타일의 Text generation 가능
- 예시: Obama speeches, Harry Potter 소설 스타일 등

# Recurrent Neural Networks for Other Applications
Tagging, classification, question answering, speech recognition

## RNNs Can Be Used for Tagging
![alt text](image-65.png)
- Part-of-speech tagging, Named Entity Recognition 등

## RNNs Can Be Used for Sentence Classification
![alt text](image-66.png)
- Sentiment classification 등

## RNN-LMs Can Be Used to Generate Text
![alt text](image-67.png)
- Speech recognition, Machine Translation, Summarization 등

# Variants of RNNs

## Bidirectional and Multi-Layer RNNs: Motivation
- 양방향 정보를 활용하거나 깊이를 더하기 위한 동기

## Bidirectional RNNs
- Forward 및 Backward 정보를 결합하여 활용

## Multi-Layer RNNs
- RNN을 여러 층으로 쌓아 구성

# Long Short-Term Memory RNNs (LSTMs)
- Hochreiter와 Schmidhuber가 1997년에 제안한 RNN의 일종
- Vanishing gradients problem에 대한 해결책으로 제시됨.

## Mathematical Formulation
Sequence inputs $x^{(t)}$가 주어졌을 때, Hidden states $h^{(t)}$와 Cell states $c^{(t)}$를 계산하는 과정은 다음과 같음. (Timestep $t$ 기준)

### 1. Gates and Cell Updates

각 게이트와 상태 업데이트를 위한 수식 ($\sigma$는 Sigmoid function, $\circ$는 Element-wise product)

Forget Gate: 이전 Cell state의 정보를 얼마나 유지할지 결정
$$f^{(t)} = \sigma(W_f h^{(t-1)} + U_f x^{(t)} + b_f)$$

Input Gate: 새로운 정보를 Cell state에 얼마나 반영할지 결정
$$i^{(t)} = \sigma(W_i h^{(t-1)} + U_i x^{(t)} + b_i)$$

Output Gate: 현재 Cell state를 바탕으로 Hidden state를 어떻게 출력할지 결정
$$o^{(t)} = \sigma(W_o h^{(t-1)} + U_o x^{(t)} + b_o)$$

New Cell Content: Cell에 기록될 새로운 후보 값
$$\tilde{c}^{(t)} = \tanh(W_c h^{(t-1)} + U_c x^{(t)} + b_c)$$

### 2. State Updates

Cell State: 이전 상태를 잊고(Forget), 새로운 내용을 추가(Input)하여 업데이트
$$c^{(t)} = f^{(t)} \circ c^{(t-1)} + i^{(t)} \circ \tilde{c}^{(t)}$$

Hidden State: 업데이트된 Cell state를 가공하여 출력
$$h^{(t)} = o^{(t)} \circ \tanh(c^{(t)})$$

### 1. The Repeating Module (Chain Structure)

표준적인 RNN과 달리, LSTM의 반복 모듈은 4개의 상호작용하는 Layer를 포함

### 2. Detailed LSTM Cell Diagram
![alt text](image-70.png)

![alt text](image-69.png)

![alt text](image-68.png)

Diagram Legend
  - `(X)`: Pointwise Multiplication (Element-wise)
  - `(+)`: Pointwise Addition (The "Secret" regarding gradients)
  - `SIGMOID`, `TANH`: Neural Network Layers (Activation Functions)
  - `->`: Vector Transfer
  - `Combine`: Concatenation