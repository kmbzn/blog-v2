# 20. Attention Mechanism and Self-Attention

# Sequence-to-Sequence Models
Focusing on neural machine translation

## Machine Translation
- **Machine Translation (MT)**
  - Source language의 문장 $x$를 Target language의 문장 $y$로 번역하는 Task
- **The early history of MT: 1950s**
  - "A.I."라는 용어가 만들어지기 전인 1950년대 초반에 시작
  - 주로 단어 치환을 수행하는 단순한 Rule-based systems
  - 자연어 Syntax, Semantics, Pragmatics에 대한 이해 부족

## Neural Machine Translation
- **Neural Machine Translation (NMT)**
  - 하나의 End-to-end Neural Network로 Machine Translation을 수행하는 방법
  - 해당 Neural Network 아키텍처는 Sequence-to-sequence model (a.k.a seq2seq)이라 불리며 두 개의 RNN을 포함

![](image-75.png)

## Sequence-to-Sequence is Versatile!
- **Encoder-Decoder model**이라는 일반적인 개념
  - 하나의 Neural Network는 입력을 받아 Neural Representation을 생성
  - 다른 Network는 해당 Neural Representation을 기반으로 출력을 생성
  - 입력과 출력이 Sequence인 경우 이를 Seq2seq model이라 칭함
- **Sequence-to-sequence**는 MT 외에도 다양한 NLP Task에 유용
  - Summarization (Long text → Short text)
  - Dialogue (Previous utterances → Next utterance)
  - Code generation (Natural language → Python code)

## Neural Machine Translation (NMT)
- Sequence-to-sequence model은 **Conditional Language Model**의 한 예시
  - Decoder가 Target sentence $y$의 다음 단어를 예측하므로 Language Model
  - 예측이 Source sentence $x$에 조건을 받기(Conditioned) 때문에 Conditional
- NMT는 $P(y|x)$를 직접 계산
  - $$P(y|x) = P(y_1|x) P(y_2|y_1, x) P(y_3|y_1, y_2, x) \dots P(y_T|y_1, \dots, y_{T-1}, x)$$
- **NMT System 학습 방법**
  - 대규모 Parallel corpus 확보
  - Unsupervised NMT, Data augmentation 등에 대한 흥미로운 연구 존재

## Training a Neural Machine Translation System
![](image-76.png)

# Introduction to Attention

## The Bottleneck Problem of Seq2Seq
![](image-77.png)

## Attention
- Attention (mechanism)은 Bottleneck 문제에 대한 해결책 제공
  - **Core idea**: Decoder의 각 단계에서 Encoder에 직접 연결하여 Source sequence의 특정 부분에 집중(Focus)
- Sequence-to-sequence with Attention

## Sequence-to-Sequence with Attention
![](image-78.png)

![](image-79.png)

![](image-80.png)

![](image-81.png)

## Attention: In Equations
- **Procedure**
  1. Encoder hidden states $h_1, \dots, h_N \in \mathbb{R}^h$ 보유
  2. Timestep $t$에서 Decoder hidden state $s_t \in \mathbb{R}^h$ 보유
  3. 이 단계의 Attention scores $e_t$ 계산
    $$e^t = [s_t^T h_1, \dots, s_t^T h_N] \in \mathbb{R}^N$$
  4. Softmax를 취해 이 단계의 Attention distribution $\alpha_t$ 획득 (확률 분포이며 합은 1)
    $$\alpha^t = \text{softmax}(e^t) \in \mathbb{R}^N$$
  5. $\alpha_t$를 사용하여 Encoder hidden states의 Weighted sum인 Attention output $a_t$ 계산
    $$a_t = \sum_{i=1}^{N} \alpha_i^t h_i \in \mathbb{R}^h$$
  6. 마지막으로 Attention output $a_t$를 Decoder hidden state $s_t$와 Concatenate하고 Non-attention seq2seq model과 같이 진행
    $$[a_t; s_t] \in \mathbb{R}^{2h}$$

## Attention is Great!
- **Attention**은 NMT 성능을 크게 향상
  - Decoder가 Source의 특정 부분에 집중하도록 하는 것이 매우 유용
- **Attention**은 MT 과정에 대해 더 "Human-like"한 Model을 제공
  - 전체를 기억할 필요 없이 번역하면서 Source sentence를 다시 볼 수 있음
- **Attention**은 Bottleneck 문제 해결
  - Decoder가 Source를 직접 볼 수 있게 하여 Bottleneck 우회
- **Attention**은 Vanishing Gradient 문제 해결에 도움
  - 멀리 떨어진 State로의 Shortcut 제공

    ![](image-82.png)
- **Attention**은 어느 정도의 Interpretability 제공
  - Attention distribution을 검사하여 Decoder가 무엇에 집중했는지 확인 가능
  - (Soft) Alignment를 무료로 획득
  - Alignment system을 명시적으로 학습하지 않았음에도 Network가 스스로 Alignment를 학습했다는 점이 흥미로움

## Attention is A General Deep Learning Technique
- **Attention**이 Machine Translation을 위한 Sequence-to-sequence model을 개선하는 훌륭한 방법임을 확인
  - 그러나, Seq2seq뿐만 아니라 다양한 아키텍처와 MT 외의 다양한 Task에서 Attention 사용 가능
- **Attention의 더 일반적인 정의**
  - Vector **Values** 집합과 Vector **Query**가 주어졌을 때, Attention은 Query에 의존하여 **Values의 Weighted sum**을 계산하는 기법
- 때때로 Query가 Values에 **Attend**한다고 표현
  - 예: Seq2seq + Attention model에서 각 Decoder hidden state (Query)는 모든 Encoder hidden states (Values)에 Attend
- **Intuition**
  - Weighted sum은 Values에 포함된 정보의 선택적 요약(Selective summary)이며, Query가 어떤 Values에 집중할지 결정
  - Attention은 임의의 Representations 집합(Values)으로부터 다른 Representation(Query)에 의존하여 Fixed-size representation을 얻는 방법
- **Conclusion**
  - Attention은 모든 Deep Learning Model에서 Pointer 및 Memory 조작을 위한 강력하고 유연하며 일반적인 방법이 됨.
  - 2010년 이후 NMT로부터 나온 새로운 아이디어

# From RNN to Attention-Based NLP Models Self-Attention

## As of Last Lecture: Recurrent Models for (Most) NLP
![](image-83.png)
- 2016-2018년경, NLP의 사실상 표준 전략은 문장을 **Bidirectional LSTM**으로 인코딩하는 것
  - 예: 번역에서의 Source sentence
- 그 후, 출력(Translation, Sentence, Summary)을 Sequence로 정의하고 생성을 위해 LSTM 사용
- 유연한 Memory 접근을 위해 **Attention** 사용

## Same Goals, Different Building Blocks
![](image-84.png)
- Sequence-to-sequence 문제와 Encoder-Decoder model에 대해 학습
  - 현재로서는 문제를 바라보는 완전히 새로운 방식을 동기부여하려는 것이 아님
  - 대신 Model에 적용하여 폭넓은 발전을 가능하게 할 최상의 **Building Blocks**를 찾으려는 것

## Issues with Recurrent Models: Linear Interaction Distance
- RNN은 "Left-to-right"로 Unrolled(입력을 받아들여 처리)됨
  - 이는 **Linear locality**를 인코딩하며 유용한 Heuristic
  - 가까운 단어들은 종종 서로의 의미에 영향을 미침
- **Problem**
  - RNN은 멀리 떨어진 단어 쌍이 상호작용하기 위해 $O(\text{sequence length})$ 단계가 필요

## Issues with Recurrent Models: Linear Interaction Distance
![](image-85.png)
- 멀리 떨어진 단어 쌍의 상호작용에 $O(\text{sequence length})$ 단계가 필요하다는 의미
  - Long-distance dependencies 학습의 어려움 (Gradient 문제 때문)
  - 단어의 Linear order가 "Baked in" 됨; 그러나 Linear order가 문장을 생각하는 최선의 방법이 아닐 수 있음

![](image-86.png)

## Issues with Recurrent Models: Lack of Parallelizability
- Forward 및 Backward pass는 $O(\text{sequence length})$의 병렬화 불가한(Unparallelizable) 연산을 포함
  - GPU는 한 번에 많은 독립적인 계산을 수행 가능
  - 그러나 미래의 RNN hidden states는 과거의 RNN hidden states가 계산되기 전까지는 완전히 계산될 수 없음
  - 이는 매우 큰 Datasets에서의 **학습을 저해**

![](image-87.png)

![](image-88.png)

## If Not Recurrence, Then What? How About Attention?
- **Attention**은 각 단어의 Representations를 Query로 취급하여 Values 집합의 정보에 접근하고 통합
  - Decoder에서 Encoder로의 Attention을 보았으나, 이제 단일 문장 내에서의 Attention을 고려 → **Self-Attention!**
  - Attention의 경우, 병렬화 불가한 연산의 수가 Sequence length에 따라 증가하지 않음
  - Maximum interaction distance: 모든 단어가 모든 Layer에서 상호작용하므로 $O(1)$
![](image-89.png)

## Attention as a Soft, Averaging Lookup Table
- **Attention**을 Key-Value store에서의 Fuzzy lookup 수행으로 생각할 수 있음

![](image-90.png)

## Self-Attention Hypothetical Example

![](image-91.png)

## Self-Attention: Keys, Queries, Values from the Same Sequence
- 단어장 $V$ 내의 단어 Sequence $w_{1:n}$ 가정
  - 예: *"Zuko made his uncle tea"*
- 각 $w_i$에 대해 $x_i = Ew_i$ (여기서 $E \in \mathbb{R}^{d \times V}$는 Embedding matrix)
1. 각 Word embedding을 Weight matrices $Q, K, V$ (각각 $\mathbb{R}^{d \times d}$)로 변환
  $$q_i = Qx_i \text{ (queries)}, \quad k_i = Kx_i \text{ (keys)}, \quad v_i = Vx_i \text{ (values)}$$
2. Keys와 Queries 간의 Pairwise similarities 계산 및 Softmax로 정규화
  $$e_{i,j} = q_i \cdot k_j, \quad \alpha_{i,j} = \frac{\exp(e_{i,j})}{\sum_{k} \exp(e_{i,k})}$$
3. 각 단어에 대한 출력을 Values의 Weighted sum으로 계산
  $$o_i = \sum_{j} \alpha_{i,j} v_j$$

## Barriers(장애물) & Solutions for Self-Attention as A Building Block
- **Barrier**: 순서에 대한 고유한 개념 부재
  - **Solution**: Input에 position representation 추가
- **Barrier**: Deep learning을 위한 nonlinearity 부재 (단지 weighted average일 뿐임)
  - **Solution**: 간단한 해결책으로 각 self-attention output에 동일한 feedforward network 적용
- **Barrier**: Sequence 예측 시 "미래를 보지(look at the future)" 않도록 보장 필요
  - Machine translation의 경우
  - 혹은 Language modeling의 경우
  - **Solution**: Attention weight를 인위적으로 0으로 설정하여 미래를 mask out